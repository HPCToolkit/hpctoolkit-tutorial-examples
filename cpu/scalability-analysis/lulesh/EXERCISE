Thread scaling analysis of Lulesh

The Makefile in this directory will run Lulesh on 16 and 32 threads
on a KNL processor. Differential profiling (comparing two executions),
as described in the reference at the bottom of this file, is a powerful
techique to pinpoint and quantify scaling losses in parallel programs
in executions within and/or across nodes.

This technique can be used for scalability analysis of either strong or
weak scaling.  (Weak scaling requires some additional coefficients to
compare executions that perform different amounts of work.)

Here, you will study a strong-scaling experiment and compare the
performance of Lulesh as it performs the same calculation with different
numbers of threads.  You will identify (1) the level of scaling losses
and (2) pinpoint where the losses occur as Lulesh is scaled from 16 to
32 threads. HPCToolkit's measurement system has no idea that you will be
performing scalability analysis of its measurement results. Scalability
analysis requires only HPCToolkit's usual measurement approach: call path
profiling of every thread in the execution using asynchronous sampling.

When you open hpcviewer on the performance database
'hpctoolkit-lulesh-16-to-32-scaling.d', you will see 4 columns. The
leftmost 2 are labeled with the prefix "1." and the rightmost two
have the prefix "2.". When multiple performance databases are merged
together by hpcprof (or hpcprof-mpi), it assigns each database an index,
in this case 1 and 2. The leftmost two columns represent the 16-thread
execution. The rightmost 2 represent the 32-thread execution.

As described in the 'Flash demo' in a talk about scalability analysis,
you will use hpcviewer's ability to compute derived metrics to compute
scaling losses.  You will compute two types of scaling losses: inclusive
and exclusive.

Above the navigation pane in the profile view, click on the 'f(x)'
icon to bring up a pane to create a derived metric. You will brin up
this pane twice, once to create each of two types of derived metrics -
the first comparing inclusive costs in the two executions and the second
comparing exclusive costs in the two executions.

In the formula bar, you will compute a formula
100 * (the cost of the 32 thread metric at any point in the calling context tree - 
       the cost of the 16 thread metric at that same point in the calling context tree) / 
      (the total cost of the 32 thread execution)

Each metric is identified by a number. In my copy of this database,
I see metrics starting
 with 96.

When you compute the equation above, you will first write a formula to
compute a differential performance metric with inclusive values. The
value of a metric at a calling context tree node is specified by $<metric
number>, e.g., $96 for metric 96, using a notation like that used to
identify cells in an excel spreadsheet. To divide by the total cost of
the 32 thread execution, you use @ instead of $ to denote the cost at
the root of the calling context tree.

Check to pull down menu to make sure you know the identity of the metrics
to use in an equation.  Each equation (assuming you see the same metric
indices that I do) will look like

100 * ($98 - $96)/@98

However, you will use different pairs of metrics to compute the inclusive
scalability losses vs. exclusive scalability losses; namely, the inclusive
and exclusive metrics from each execution.

Name the inclusive scalability loss metric "scalability loss (I)",
select the button to "display the metric value as a percent", and click
OK. Bring up the pane again to create an exclusive scalability loss metric
"scalability loss (E)" using exclusive values.

The inclusive scalability loss metric is useful in the top-down view for
(1) seeing what the total percent of scalability loss is for the whole
calling context tree at the root, and (2) using the flame button to dig
down call paths to see where losses occur. You can dig down paths in the
tree by selecting any node in the tree and clicking on the flame button
to follow losses down the tree until a child doesn't account for most
of its parent's losses.

Q1: How much is the total scalability loss?

Q2: Does the program scale well and have high parallel efficiency?

The most powerful way to analyze scalability losses is in the bottom-up
view. Select the exclusive scalability loss metric in the bottom-up view,
to sort procedures by that metric.

Q3: What function or placeholder appears at the top of the column as
being responsible for the most losses?

Select that function and 'open its triangle' to reveal paths up through
its callers.

Q4: Where do the losses occur? Do they make sense?

Here, we compute scalability losses between executions with different
numbers of threads. You can do the same thing with different numbers of
ranks in an MPI program. You can do the same with different ranks and/or
threads with an MPI+OpenMP program.

NOTE: top-down scalability analysis works best when the calling context
measurements look like a tree rather than a forest. (Without the OMPT
interface for OpenMP, costs for the master thread and the worker threads
will not be combined into a single tree; there will be separate costs
for <program root> and <thread root> so the result is a forest.) When
your performance data is for a forest, only the bottom-up scalability
analysis will typically be useful.




Reference:

Cristian Coarfa, John Mellor-Crummey, Nathan Froyd, and Yuri
Dotsenko. 2007. Scalability analysis of SPMD codes using expectations. In
Proceedings of the 21st annual international conference on Supercomputing
(ICS '07). Association for Computing Machinery, New York, NY, USA,
13-22. DOI:https://doi.org/10.1145/1274971.1274976. Available as
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.1372&rep=rep1&type=pdf
